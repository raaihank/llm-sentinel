upstream:
  ollama: http://host.docker.internal:11434

security:
  mode: log
  rate_limit:
    enabled: true
    requests_per_min: 120
    max_request_size: 2097152  # 2MB
    burst_limit: 20
  vector_security:
    enabled: true
    # service_type is defined under embedding.service_type
    block_threshold: 0.7
    max_batch_size: 32
    embedding:
      service_type: "ml"
      redis_enabled: true
      redis_url: "redis:6379"
      redis_db: 0
      model:
        model_name: "all-MiniLM-L6-v2"
        model_path: ""
        tokenizer_path: ""
        vocab_path: ""
        cache_dir: "/models"
        auto_download: true
        max_length: 512
        batch_size: 8
        model_timeout: 60s
        cache_ttl: 6h
    database:
      database_url: "postgres://sentinel:sentinel_pass@postgres:5432/llm_sentinel?sslmode=disable"
      max_open_conns: 20
      max_idle_conns: 10
      conn_max_lifetime: 1h
      conn_max_idle_time: 30m
